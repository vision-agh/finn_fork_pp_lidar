{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Brevitas Export, FINN Import and Tidy-Up\n",
    "\n",
    "Similar to what we did in the TFC-w1a1 end-to-end notebook, we will start by exporting the [pretrained CNV-w1a1 network](https://github.com/maltanar/brevitas_cnv_lfc) to ONNX, importing that into FINN and running the \"tidy-up\" transformations to have a first look at the topology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.util.basic import make_build_dir\n",
    "from finn.util.visualization import showInNetron\n",
    "\n",
    "#TODO: Make RPN to load config from xyres16.proto\n",
    "\n",
    "build_dir = \"/workspace/finn\"\n",
    "base_file_name = \"rpn\"\n",
    "config_path = \"/workspace/finn/pointpillars/second/configs/pointpillars/car/xyres_16.proto\"\n",
    "#in_shape = (1,64,496,432) - this is the real input shape, but FINN gets only symmetric tensors, FUCK!\n",
    "#in_shape = (1,64,432,432)\n",
    "in_shape = (1,64,320,320)\n",
    "\n",
    "import onnx\n",
    "from finn.util.test import get_test_model_trained\n",
    "import brevitas.onnx as bo\n",
    "from finn.core.modelwrapper import ModelWrapper\n",
    "from finn.transformation.double_to_single_float import DoubleToSingleFloat\n",
    "from finn.transformation.infer_shapes import InferShapes\n",
    "from finn.transformation.fold_constants import FoldConstants\n",
    "from finn.transformation.general import GiveReadableTensorNames, GiveUniqueNodeNames\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from brevitas.quant_tensor import pack_quant_tensor\n",
    "import brevitas.nn as qnn\n",
    "from brevitas.core.quant import QuantType\n",
    "from brevitas.core.restrict_val import RestrictValueType\n",
    "from brevitas.core.scaling import ScalingImplType\n",
    "from brevitas.core.stats import StatsOp\n",
    "\n",
    "\n",
    "from second.pytorch.builder import second_builder\n",
    "from second.pytorch.models.quantization import QuantConfig\n",
    "from torchplus.tools import change_default_args\n",
    "from second.pytorch.models.quantization import MyQuantReLU\n",
    "import torchplus\n",
    "from second.protos import pipeline_pb2\n",
    "\n",
    "QuantConfig.BACKBONE_CONV_QUANT_TYPE = QuantType.BINARY\n",
    "QuantConfig.BACKBONE_CONV_BIT_WIDTH  = 1\n",
    "\n",
    "QuantConfig.LAST_LAYER_QUANT_TYPE = QuantType.INT\n",
    "QuantConfig.LAST_LAYER_BIT_WIDTH  = 8\n",
    "\n",
    "QuantConfig.ACTIVATION_QUANT_TYPE = QuantType.INT\n",
    "QuantConfig.ACTIVATION_BIT_WIDTH  = 2\n",
    "QuantConfig.ACTIVATION_FUNCTION   = change_default_args(\n",
    "    max_val           = 6,\n",
    "    quant_type        = QuantConfig.ACTIVATION_QUANT_TYPE, \n",
    "    bit_width         = QuantConfig.ACTIVATION_BIT_WIDTH, \n",
    "    scaling_impl_type = ScalingImplType.CONST)(MyQuantReLU)\n",
    "\n",
    "    \n",
    "import onnx\n",
    "from finn.util.test import get_test_model_trained\n",
    "import brevitas.onnx as bo\n",
    "from google.protobuf import text_format\n",
    "\n",
    "config = pipeline_pb2.TrainEvalPipelineConfig()\n",
    "with open(config_path, \"r\") as f:\n",
    "    proto_str = f.read()\n",
    "    text_format.Merge(proto_str, config)\n",
    "input_cfg = config.train_input_reader\n",
    "eval_input_cfg = config.eval_input_reader\n",
    "model_cfg = config.model.second\n",
    "train_cfg = config.train_config\n",
    "\n",
    "from second.pytorch.models.voxelnet import RPN\n",
    "\n",
    "rpn = RPN(\n",
    "     use_norm                   = True,\n",
    "     num_class                  = 1,\n",
    "     layer_nums                 = [3, 3, 3],\n",
    "     layer_strides              = [2, 1, 1],\n",
    "     num_filters                = [64, 128, 256],\n",
    "     upsample_strides           = [1, 1, 1],\n",
    "     num_upsample_filters       = [128, 128, 128],\n",
    "     num_input_filters          = 64,\n",
    "     num_anchor_per_loc         = 2,\n",
    "     encode_background_as_zeros = True,\n",
    "     use_direction_classifier   = True,\n",
    "     use_groupnorm              = False,\n",
    "     num_groups                 = 32,\n",
    "     use_bev                    = False,\n",
    "     box_code_size              = 7,\n",
    ")\n",
    "checkpoint_loc = \"/workspace/finn/pp_net_params/rpn_weights\"\n",
    "checkpoint = torch.load(checkpoint_loc, map_location=\"cpu\")\n",
    "rpn.load_state_dict(checkpoint)\n",
    "rpn = rpn.eval()\n",
    "bo.export_finn_onnx(rpn, in_shape, build_dir + \"/{}.onnx\".format(base_file_name))\n",
    "\n",
    "model = ModelWrapper(build_dir + \"/{}.onnx\".format(base_file_name))\n",
    "model = model.transform(DoubleToSingleFloat())\n",
    "model = model.transform(InferShapes())\n",
    "model = model.transform(FoldConstants())\n",
    "model = model.transform(GiveUniqueNodeNames())\n",
    "model = model.transform(GiveReadableTensorNames())\n",
    "model.save(build_dir + \"/{}_tidy.onnx\".format(base_file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is exported, let's have a look at its layer structure with Netron. Remember that the visualization below is interactive, you can click on the individual nodes and view the layer attributes, trained weights and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving '/workspace/finn/rpn_tidy.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://0.0.0.0:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f7d8c434860>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(build_dir + \"/{}_tidy.onnx\".format(base_file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the network is composed of a repeating convolution-convolution-maxpool layer pattern to extract features using 3x3 convolution kernels (with weights binarized) and `Sign` activations, followed by fully connected layers acting as the classifier. Also notice the initial `MultiThreshold` layer at the beginning of the network, which is quantizing float inputs to 8-bit ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. How FINN Implements Convolutions: Lowering and Streamlining\n",
    "\n",
    "In FINN, we implement convolutions with the *lowering* approach: we convert them to matrix-matrix multiply operations, where one of the matrices is generated by sliding a window over the input image. You can read more about the sliding window operator and how convolution lowering works [in this notebook](https://github.com/maltanar/qnn-inference-examples/blob/master/3-convolutional-binarized-gtsrb.ipynb). The streaming dataflow architecture we will end up with is going to look something like this figure from the [FINN-R paper](https://arxiv.org/abs/1809.04570):\n",
    "\n",
    "![](cnv-mp-fc.png)\n",
    "\n",
    "Note how the convolution layer looks very similar to the fully connected one in terms of the matrix-vector-threshold unit (MVTU), but now the MVTU is preceded by a sliding window unit that produces the matrix from the input image. All of these building blocks, including the `MaxPool` layer you see in this figure, exist as templated Vivado HLS C++ functions in [finn-hlslib](https://github.com/Xilinx/finn-hlslib).\n",
    "\n",
    "\n",
    "To target this kind of hardware architecture with our network we'll apply a convolution lowering transformation, in addition to streamlining. You may recall the *streamlining transformation* that we applied to the TFC-w1a1 network, which is a series of mathematical simplifications that allow us to get rid of floating point scaling operations by implementing few-bit activations as thresholding operations. **The current implementation of streamlining is highly network-specific and may not work for your network if its topology is very different than the example network here. We hope to rectify this in future releases.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/finn/src/finn/transformation/streamline/absorb.py:110: RuntimeWarning: overflow encountered in true_divide\n",
      "  Tnew = T / A.reshape(-1, 1)\n"
     ]
    }
   ],
   "source": [
    "from finn.transformation.streamline import Streamline\n",
    "from finn.transformation.lower_convs_to_matmul import LowerConvsToMatMul\n",
    "from finn.transformation.bipolar_to_xnor import ConvertBipolarMatMulToXnorPopcount\n",
    "import finn.transformation.streamline.absorb as absorb\n",
    "from finn.transformation.streamline.reorder import MakeMaxPoolNHWC, MoveAddMulPastIm2Col\n",
    "\n",
    "model = ModelWrapper(build_dir + \"/{}_tidy.onnx\".format(base_file_name))\n",
    "model = model.transform(Streamline())\n",
    "model.save(build_dir + \"/{}_streamline_tmp_1.onnx\".format(base_file_name))\n",
    "model = model.transform(LowerConvsToMatMul())\n",
    "model.save(build_dir + \"/{}_streamline_tmp_2.onnx\".format(base_file_name))\n",
    "model = model.transform(MakeMaxPoolNHWC())\n",
    "model = model.transform(MoveAddMulPastIm2Col())\n",
    "model.save(build_dir + \"/{}_streamline_tmp_3.onnx\".format(base_file_name))\n",
    "model = model.transform(absorb.AbsorbTransposeIntoMultiThreshold())\n",
    "model.save(build_dir + \"/{}_streamline_tmp_4.onnx\".format(base_file_name))\n",
    "model = model.transform(ConvertBipolarMatMulToXnorPopcount())\n",
    "model.save(build_dir + \"/{}_streamline_tmp_5.onnx\".format(base_file_name))\n",
    "model = model.transform(Streamline())\n",
    "model.save(build_dir + \"/{}_streamlined.onnx\".format(base_file_name))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We won't go into too much detail about what happens in each transformation and why they are called in the particular order they are (feel free to visualize the intermediate steps using Netron yourself if you are curious) but here is a brief summmmary:\n",
    "\n",
    "* `Streamline` moves floating point scaling and addition operations closer to the input of the nearest thresholding activation and absorbs them into thresholds\n",
    "* `LowerConvsToMatMul` converts ONNX `Conv` nodes into sequences of `Im2Col, MatMul` nodes as discussed above. `Im2Col` is a custom FINN ONNX high-level node type that implements the sliding window operator.\n",
    "* `MakeMaxPoolNHWC` and `AbsorbTransposeIntoMultiThreshold` convert the *data layout* of the network into the NHWC data layout that finn-hlslib primitives use. NCHW means the tensor dimensions are ordered as `(N : batch, H : height, W : width, C : channels)` (assuming 2D images). The ONNX standard ops normally use the NCHW layout, but the ONNX intermediate representation itself does not dictate any data layout.\n",
    "* You may recall `ConvertBipolarMatMulToXnorPopcount` from the TFC-w1a1 example, which is needed to implement bipolar-by-bipolar (w1a1) networks correctly using finn-hlslib.\n",
    "\n",
    "Let's visualize the streamlined and lowered network with Netron. Observe how all the `Conv` nodes have turned into pairs of `Im2Col, MatMul` nodes, and many nodes including `BatchNorm, Mul, Add` nodes have disappeared and replaced with `MultiThreshold` nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stopping http://0.0.0.0:8081\n",
      "Serving '/workspace/finn/rpn_streamline_tmp_1.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://0.0.0.0:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f7cf78d1fd0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(build_dir + \"/{}_streamline_tmp_1.onnx\".format(base_file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showInNetron(build_dir + \"/{}_streamlined.onnx\".format(base_file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Partitioning, Conversion to HLS Layers and Folding\n",
    "\n",
    "The next steps will be (again) very similar to what we did for the TFC-w1a1 network. We'll first convert the layers that we can put into the FPGA into their HLS equivalents and separate them out into a *dataflow partition*:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import finn.transformation.fpgadataflow.convert_to_hls_layers as to_hls\n",
    "from finn.transformation.fpgadataflow.create_dataflow_partition import (\n",
    "    CreateDataflowPartition,\n",
    ")\n",
    "from finn.transformation.move_reshape import RemoveCNVtoFCFlatten\n",
    "from finn.custom_op.registry import getCustomOp\n",
    "\n",
    "# choose the memory mode for the MVTU units, decoupled or const\n",
    "mem_mode = \"decoupled\"\n",
    "#mem_mode = \"MVTU\"\n",
    "\n",
    "model = ModelWrapper(build_dir + \"/{}_streamlined.onnx\".format(base_file_name))\n",
    "model = model.transform(to_hls.InferBinaryStreamingFCLayer(mem_mode))\n",
    "model = model.transform(to_hls.InferQuantizedStreamingFCLayer(mem_mode))\n",
    "#model.save(build_dir + \"/{}_dataflow_tmp_1.onnx\".format(base_file_name))\n",
    "#model = ModelWrapper(build_dir + \"/{}_dataflow_tmp_1.onnx\".format(base_file_name))\n",
    "model = model.transform(to_hls.InferConvInpGen())\n",
    "model = model.transform(to_hls.InferStreamingMaxPool())\n",
    "# get rid of Reshape(-1, 1) operation between hlslib nodes\n",
    "model = model.transform(RemoveCNVtoFCFlatten())\n",
    "parent_model = model.transform(CreateDataflowPartition())\n",
    "parent_model.save(build_dir + \"/{}_dataflow_parent.onnx\".format(base_file_name))\n",
    "sdp_node = parent_model.get_nodes_by_op_type(\"StreamingDataflowPartition\")[0]\n",
    "sdp_node = getCustomOp(sdp_node)\n",
    "dataflow_model_filename = sdp_node.get_nodeattr(\"model\")\n",
    "# save the dataflow partition with a different name for easier access\n",
    "dataflow_model = ModelWrapper(dataflow_model_filename)\n",
    "dataflow_model.save(build_dir + \"/{}_dataflow_model.onnx\".format(base_file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving '/workspace/finn/rpn_dataflow_parent.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://0.0.0.0:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f7ab4154048>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(build_dir + \"/{}_dataflow_parent.onnx\".format(base_file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 20, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "dbg = True\n",
    "if dbg:\n",
    "    import finn.custom_op.registry as registry\n",
    "    import numpy as np\n",
    "    op_type = \"Mul\"\n",
    "    \n",
    "    model = ModelWrapper(build_dir + \"/{}_dataflow_parent.onnx\".format(base_file_name))\n",
    "    nodes = model.get_nodes_by_op_type(op_type)\n",
    "    #print(nodes)\n",
    "    node = nodes[0]\n",
    "    \n",
    "    if op_type == \"MultiThreshold\":\n",
    "        inst = registry.custom_op[op_type](node)\n",
    "        thresholds = model.get_initializer(node.input[1])\n",
    "        out_scale  = inst.get_nodeattr(\"out_scale\")\n",
    "        out_bias   = inst.get_nodeattr(\"out_bias\")\n",
    "        data_layout = inst.get_nodeattr(\"data_layout\")\n",
    "        print(\"data layout (if other than NCHW, then check MultThreshold class code): {}\".format(data_layout))\n",
    "        print(\"out_scale: {}\".format(type(out_scale), out_scale))\n",
    "        print(\"out_bias: {}\".format(type(out_bias), out_bias))\n",
    "        np.save(\"{}/pp_net_params/thresholds.npy\".format(build_dir), thresholds)\n",
    "    elif op_type == \"Add\":\n",
    "        tensor = model.get_initializer(node.input[1])\n",
    "        print(tensor.shape)\n",
    "        np.save(\"{}/pp_net_params/add_params.npy\".format(build_dir), tensor)\n",
    "    elif op_type == \"Mul\":\n",
    "        tensor = model.get_initializer(node.input[1])\n",
    "        print(tensor.shape)\n",
    "        np.save(\"{}/pp_net_params/mul_params.npy\".format(build_dir), tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "showInNetron(build_dir + \"/{}_dataflow_model.onnx\".format(base_file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to set the *folding factors* for certain layers to adjust the performance of our accelerator, similar to the TFC-w1a1 example. We'll also set the desired FIFO depths around those layers, which are important to achieve full throughput in the accelerator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "Estimation of used resources: {'BRAM_18K': 196.0, 'LUT': 12629.600000000002}\n"
     ]
    }
   ],
   "source": [
    "from finn.transformation.fpgadataflow.insert_dwc import InsertDWC\n",
    "from finn.transformation.fpgadataflow.insert_tlastmarker import InsertTLastMarker\n",
    "from finn.transformation.fpgadataflow.insert_fifo import InsertFIFO\n",
    "from finn.transformation.fpgadataflow.annotate_resources import AnnotateResources\n",
    "\n",
    "model = ModelWrapper(build_dir + \"/{}_dataflow_model.onnx\".format(base_file_name))\n",
    "fc_layers = model.get_nodes_by_op_type(\"StreamingFCLayer_Batch\")\n",
    "print(len(fc_layers))\n",
    "\n",
    "# each tuple is (PE, SIMD, in_fifo_depth, ram_style) for a layer\n",
    "# there are 17 StreamingFCLayer_Batch\n",
    "PEs   = 16\n",
    "SIMDs = 16\n",
    "FIFOs = 256\n",
    "#At 16, 32, 64 we had 988% Slice utlization\n",
    "folding = [\n",
    "    (PEs, SIMDs, FIFOs, \"block\"), #0\n",
    "    (PEs, SIMDs, FIFOs, \"block\"),\n",
    "    (PEs, SIMDs, FIFOs, \"block\"),\n",
    "    (PEs, SIMDs, FIFOs, \"block\"),\n",
    "    (PEs, SIMDs, FIFOs, \"block\"), #4\n",
    "    \n",
    "    (PEs, SIMDs, FIFOs, \"block\"), #5\n",
    "    (PEs, SIMDs, FIFOs, \"block\"),\n",
    "    (PEs, SIMDs, FIFOs, \"block\"),\n",
    "    (PEs, SIMDs, FIFOs, \"block\"),\n",
    "    (PEs, SIMDs, FIFOs, \"block\"), #9\n",
    "    \n",
    "    (PEs, SIMDs, FIFOs, \"block\"), #10\n",
    "#     (PEs, SIMDs, FIFOs, \"block\"),\n",
    "#     (PEs, SIMDs, FIFOs, \"block\"),\n",
    "#     (PEs, SIMDs, FIFOs, \"block\"),\n",
    "#     (PEs, SIMDs, FIFOs, \"block\"), #14\n",
    "    \n",
    "    (PEs, SIMDs, FIFOs, \"block\"), #15\n",
    "    (1,   SIMDs, FIFOs, \"block\"), #16\n",
    "]\n",
    "for fcl, (pe, simd, ififodepth, ram_style) in zip(fc_layers, folding):\n",
    "    fcl_inst = getCustomOp(fcl)\n",
    "    fcl_inst.set_nodeattr(\"PE\", pe)\n",
    "    fcl_inst.set_nodeattr(\"SIMD\", simd)\n",
    "    fcl_inst.set_nodeattr(\"inFIFODepth\", ififodepth)\n",
    "    fcl_inst.set_nodeattr(\"ram_style\", ram_style) #default is auto\n",
    "    \n",
    "\n",
    "# use same SIMD values for the sliding window operators\n",
    "swg_layers = model.get_nodes_by_op_type(\"ConvolutionInputGenerator\")\n",
    "for i in range(len(swg_layers)):\n",
    "    swg_inst = getCustomOp(swg_layers[i])\n",
    "    simd = folding[i][1]\n",
    "    swg_inst.set_nodeattr(\"SIMD\", simd)\n",
    "\n",
    "model = model.transform(InsertDWC())\n",
    "model = model.transform(InsertFIFO())\n",
    "model = model.transform(InsertTLastMarker())\n",
    "model = model.transform(GiveUniqueNodeNames())\n",
    "model = model.transform(AnnotateResources(\"estimate\"))\n",
    "model.save(build_dir + \"/{}_folded.onnx\".format(base_file_name))\n",
    "print(\"Estimation of used resources: {}\".format(model.get_metadata_prop(\"res_total_estimate\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we visualize in Netron to observe the `StreamingDataWidthConverter` and `StreamingFIFO` nodes that have been inserted into graph, as well as the folding factors in the `PE` and `SIMD` attributes of each `StreamingFCLayer_Batch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showInNetron(build_dir + \"/{}_folded.onnx\".format(base_file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our network is now ready and we can start with the hardware generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hardware Generation\n",
    "\n",
    "From this point onward, the steps we have to follow do not depend on the particular network and will be exactly the same as the TFC-w1a1 example. We first proceed with HLS synthesis, **which may take 10-20 minutes depending on your host computer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PrepareIP started!\n",
      "Preparing IP took 67.09\n",
      "HLSSynth started!\n",
      "HLS Synthesis took 378.66 seconds\n"
     ]
    }
   ],
   "source": [
    "from finn.transformation.fpgadataflow.prepare_ip import PrepareIP\n",
    "from finn.transformation.fpgadataflow.hlssynth_ip import HLSSynthIP\n",
    "from finn.util.basic import pynq_part_map\n",
    "import time\n",
    "import traceback\n",
    "\n",
    "test_pynq_board = \"ZCU104\"\n",
    "test_fpga_part = pynq_part_map[test_pynq_board]\n",
    "target_clk_ns = 5\n",
    "\n",
    "time_start = time.time()\n",
    "try:\n",
    "    model = ModelWrapper(build_dir + \"/{}_folded.onnx\".format(base_file_name))\n",
    "    print(\"PrepareIP started!\")\n",
    "    model = model.transform(PrepareIP(test_fpga_part, target_clk_ns))\n",
    "    print(\"Preparing IP took {:.2f}\".format(time.time() - time_start))\n",
    "    time_start = time.time()\n",
    "    print(\"HLSSynth started!\")\n",
    "    model = model.transform(HLSSynthIP(0))\n",
    "    model.save(build_dir + \"/{}_ipgen.onnx\".format(base_file_name))\n",
    "except Exception as e:\n",
    "    print(\"Exception: {}\\n {}\".format(e, traceback.format_exc()))\n",
    "print(\"HLS Synthesis took {:.2f} seconds\".format(time.time() - time_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimation of used resources (HLS): {'BRAM_18K': 0.0, 'FF': 60681.0, 'LUT': 149612.0, 'DSP48E': 0.0, 'URAM': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/finn/src/finn/analysis/fpgadataflow/hls_synth_res_estimation.py:75: UserWarning: Could not find report files, values will be set to zero\n",
      "                        for this node. Please run \"PrepareIP\" transformation and\n",
      "                        \"HLSSynthIP\" first to generate the report files\n",
      "  \"HLSSynthIP\" first to generate the report files\"\"\"\n"
     ]
    }
   ],
   "source": [
    "model = ModelWrapper(build_dir + \"/{}_ipgen.onnx\".format(base_file_name))\n",
    "model = model.transform(AnnotateResources(\"hls\"))\n",
    "print(\"Estimation of used resources (HLS): {}\".format(model.get_metadata_prop(\"res_total_hls\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the HLS synthesis is complete, we can stitch together the generated IP blocks into a larger IP that is the implementation of our network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.transformation.fpgadataflow.replace_verilog_relpaths import (\n",
    "    ReplaceVerilogRelPaths,\n",
    ")\n",
    "from finn.transformation.fpgadataflow.create_stitched_ip import CreateStitchedIP\n",
    "\n",
    "model = ModelWrapper(build_dir + \"/{}_ipgen.onnx\".format(base_file_name))\n",
    "model = model.transform(ReplaceVerilogRelPaths())\n",
    "model = model.transform(CreateStitchedIP(test_fpga_part))\n",
    "model.save(build_dir + \"/{}_ipstitch.onnx\".format(base_file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create a PYNQ project that includes the hardware \"shell\" that will support our accelerator, including the data movers, and run Vivado synthesis, **which may take around 30 minutes depending on your host computer.**\n",
    "\n",
    "*If you'd like to watch the progress, you can open the generated project file (printed below) with the Vivado GUI.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vivado synthesis project is at /tmp/finn_dev_konradl/vivado_pynq_proj_nqo2_yv6/resizer.xpr\n"
     ]
    }
   ],
   "source": [
    "from finn.transformation.fpgadataflow.make_pynq_proj import MakePYNQProject\n",
    "from finn.transformation.fpgadataflow.synth_pynq_proj import SynthPYNQProject\n",
    "\n",
    "model = ModelWrapper(build_dir + \"/{}_ipstitch.onnx\".format(base_file_name))\n",
    "model = model.transform(MakePYNQProject(test_pynq_board))\n",
    "vivado_proj = model.get_metadata_prop(\"vivado_pynq_proj\")\n",
    "print(\"Vivado synthesis project is at %s/resizer.xpr\" % vivado_proj)\n",
    "model.save(build_dir + \"/{}_pynqproj.onnx\".format(base_file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vivado project Synthesis took 212.03 seconds\n"
     ]
    }
   ],
   "source": [
    "model = ModelWrapper(build_dir + \"/{}_pynqproj.onnx\".format(base_file_name))\n",
    "time_start = time.time()\n",
    "try:\n",
    "    model = model.transform(SynthPYNQProject())\n",
    "    model.save(build_dir + \"/{}_synth.onnx\".format(base_file_name))\n",
    "except Exception as e:\n",
    "    print(\"Exception: {}\\n {}\".format(e, traceback.format_exc()))\n",
    "print(\"Vivado project Synthesis took {:.2f} seconds\".format(time.time() - time_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Deployment and Remote Execution\n",
    "\n",
    "Now that we're done with the hardware generation, we can generate a Python driver for accelerator and copy the necessary files onto our PYNQ board."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from finn.transformation.fpgadataflow.make_pynq_driver import MakePYNQDriver\n",
    "from finn.transformation.fpgadataflow.make_deployment import DeployToPYNQ\n",
    "from finn.util.basic import make_build_dir\n",
    "from finn.util.visualization import showInNetron\n",
    "from finn.core.modelwrapper import ModelWrapper\n",
    "from finn.custom_op.registry import getCustomOp\n",
    "build_dir = \"/workspace/finn\"\n",
    "base_file_name = \"rpn\"\n",
    "\n",
    "# set up the following values according to your own environment\n",
    "# FINN will use ssh to deploy and run the generated accelerator\n",
    "# ip = os.getenv(\"PYNQ_IP\", \"192.168.1.99\")\n",
    "ip = \"192.168.2.99\"\n",
    "username = os.getenv(\"PYNQ_USERNAME\", \"xilinx\")\n",
    "password = os.getenv(\"PYNQ_PASSWORD\", \"xilinx\")\n",
    "port = os.getenv(\"PYNQ_PORT\", 22)\n",
    "target_dir = os.getenv(\"PYNQ_TARGET_DIR\", \"/home/xilinx/finn\")\n",
    "\n",
    "model = ModelWrapper(build_dir + \"/{}_synth.onnx\".format(base_file_name))\n",
    "print(\"1\")\n",
    "model = model.transform(MakePYNQDriver())\n",
    "print(\"2\")\n",
    "model = model.transform(DeployToPYNQ(ip, port, username, password, target_dir))\n",
    "print(\"3\")\n",
    "deploy_dir = model.get_metadata_prop(\"pynq_deploy_dir\")\n",
    "print(\"4\")\n",
    "model.save(build_dir + \"/{}_pynq_deploy.onnx\".format(base_file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sshpass -p {password} ssh {username}@{ip} -p {port} 'ls -l {target_dir}/*'\n",
    "print(deploy_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only have two more steps to be able to remotely execute the deployed bitfile with some test data from the CIFAR-10 dataset. Let's load up some test data that comes bundled with FINN -- and before you ask, that's supposed to be a cat (CIFAR-10 class number 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkg_resources as pk\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# fn = pk.resource_filename(\"finn\", \"data/cifar10/cifar10-test-data-class3.npz\")\n",
    "# x = np.load(fn)[\"arr_0\"].astype(np.float32)\n",
    "# x = x / 255\n",
    "# plt.imshow(x.reshape(3, 32,32).transpose(1, 2, 0))\n",
    "\n",
    "x = np.ones((1,64,320,320)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we partitioned our original network into a parent graph that contained the non-synthesizable nodes and a child graph that contained the bulk of the network, which we turned into a bitfile. We'll load up the parent graph, modify the `StreamingDataflowPartition` node so that it points to the deployed ONNX graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# point to the PYNQ-deployed model as the StreamingDataflowPartition in the parent\n",
    "parent_model = ModelWrapper(build_dir+\"/{}_dataflow_parent.onnx\".format(base_file_name))\n",
    "sdp_node = parent_model.get_nodes_by_op_type(\"StreamingDataflowPartition\")[0]\n",
    "sdp_node = getCustomOp(sdp_node)\n",
    "sdp_node.set_nodeattr(\"model\", build_dir + \"/rpn_pynq_deploy.onnx\")\n",
    "parent_model.save(build_dir+\"/rpn_dataflow_parent_with_remote_bitfile_exec.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can call `execute_onnx` on the parent graph, which will internally call remote execution with the bitfile once the `StreamingDataflowPartition` node is reached, grab the results, then continue executing the last portion of the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from finn.core.onnx_exec import execute_onnx\n",
    "iname = parent_model.graph.input[0].name\n",
    "oname = parent_model.graph.output[0].name\n",
    "ishape = parent_model.get_tensor_shape(iname)\n",
    "input_dict = {iname: x.reshape(ishape)}\n",
    "parent_model.set_metadata_prop(\"pynq_ip\", ip)\n",
    "parent_model.set_metadata_prop(\"pynq_port\", str(port))\n",
    "parent_model.set_metadata_prop(\"pynq_username\", username)\n",
    "parent_model.set_metadata_prop(\"pynq_password\", password)\n",
    "parent_model.set_metadata_prop(\"pynq_target_dir\", target_dir)\n",
    "parent_model.set_metadata_prop(\"pynq_deploy_dir\", deploy_dir)\n",
    "print(parent_model.get_metadata_prop(\"pynq_ip\"))\n",
    "print(parent_model.get_metadata_prop(\"pynq_port\"))\n",
    "print(parent_model.get_metadata_prop(\"pynq_username\"))\n",
    "print(parent_model.get_metadata_prop(\"pynq_password\"))\n",
    "print(parent_model.get_metadata_prop(\"pynq_target_dir\"))\n",
    "print(parent_model.get_metadata_prop(\"pynq_deploy_dir\"))\n",
    "ret = execute_onnx(parent_model, input_dict, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll pass the output of the network through a softmax function to interpret it as probabilities, and plot the per-class probabilities as a bar chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ret.shape)\n",
    "\n",
    "# def softmax(x):\n",
    "#     \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "#     e_x = np.exp(x - np.max(x))\n",
    "#     return e_x / e_x.sum()\n",
    "\n",
    "# logits = ret[oname].flatten()\n",
    "# prob = softmax(logits)\n",
    "\n",
    "# classes = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
    "\n",
    "# plt.figure(figsize=(20, 3)) \n",
    "# plt.bar(classes, prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the network correctly predicts this as a class 3 (\"cat\") with high probability. This concludes our tutorial on how to take a convolutional BNN all the way down to hardware with FINN, and execute it remotely on a PYNQ board."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
