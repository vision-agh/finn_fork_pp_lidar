{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.util.basic import make_build_dir\n",
    "from finn.util.visualization import showInNetron\n",
    "\n",
    "\n",
    "build_dir = \"/workspace/finn\"\n",
    "base_file_name = \"rpn\"\n",
    "config_path = \"/workspace/finn/pointpillars/second/configs/pointpillars/car/xyres_16.proto\"\n",
    "in_shape = (1,64,320,320)\n",
    "\n",
    "import onnx\n",
    "from finn.util.test import get_test_model_trained\n",
    "import brevitas.onnx as bo\n",
    "from finn.core.modelwrapper import ModelWrapper\n",
    "from finn.transformation.double_to_single_float import DoubleToSingleFloat\n",
    "from finn.transformation.infer_shapes import InferShapes\n",
    "from finn.transformation.fold_constants import FoldConstants\n",
    "from finn.transformation.general import GiveReadableTensorNames, GiveUniqueNodeNames\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from brevitas.quant_tensor import pack_quant_tensor\n",
    "import brevitas.nn as qnn\n",
    "from brevitas.core.quant import QuantType\n",
    "from brevitas.core.restrict_val import RestrictValueType\n",
    "from brevitas.core.scaling import ScalingImplType\n",
    "from brevitas.core.stats import StatsOp\n",
    "\n",
    "\n",
    "from second.pytorch.builder import second_builder\n",
    "from second.pytorch.models.quantization import QuantConfig\n",
    "from torchplus.tools import change_default_args\n",
    "from second.pytorch.models.quantization import MyQuantReLU\n",
    "import torchplus\n",
    "from second.protos import pipeline_pb2\n",
    "\n",
    "QuantConfig.BACKBONE_CONV_QUANT_TYPE = QuantType.BINARY\n",
    "QuantConfig.BACKBONE_CONV_BIT_WIDTH  = 1\n",
    "\n",
    "QuantConfig.LAST_LAYER_QUANT_TYPE = QuantType.INT\n",
    "QuantConfig.LAST_LAYER_BIT_WIDTH  = 8\n",
    "\n",
    "QuantConfig.ACTIVATION_QUANT_TYPE = QuantType.INT\n",
    "QuantConfig.ACTIVATION_BIT_WIDTH  = 2\n",
    "QuantConfig.ACTIVATION_FUNCTION   = change_default_args(\n",
    "    max_val           = 6,\n",
    "    quant_type        = QuantConfig.ACTIVATION_QUANT_TYPE, \n",
    "    bit_width         = QuantConfig.ACTIVATION_BIT_WIDTH, \n",
    "    scaling_impl_type = ScalingImplType.CONST)(MyQuantReLU)\n",
    "\n",
    "    \n",
    "import onnx\n",
    "from finn.util.test import get_test_model_trained\n",
    "import brevitas.onnx as bo\n",
    "from google.protobuf import text_format\n",
    "\n",
    "config = pipeline_pb2.TrainEvalPipelineConfig()\n",
    "with open(config_path, \"r\") as f:\n",
    "    proto_str = f.read()\n",
    "    text_format.Merge(proto_str, config)\n",
    "input_cfg = config.train_input_reader\n",
    "eval_input_cfg = config.eval_input_reader\n",
    "model_cfg = config.model.second\n",
    "train_cfg = config.train_config\n",
    "\n",
    "from second.pytorch.models.voxelnet import RPN\n",
    "\n",
    "rpn = RPN(\n",
    "     use_norm                   = True,\n",
    "     num_class                  = 1,\n",
    "     layer_nums                 = [3, 3, 3],\n",
    "     layer_strides              = [2, 1, 1],\n",
    "     num_filters                = [64, 128, 256],\n",
    "     upsample_strides           = [1, 1, 1],\n",
    "     num_upsample_filters       = [128, 128, 128],\n",
    "     num_input_filters          = 64,\n",
    "     num_anchor_per_loc         = 2,\n",
    "     encode_background_as_zeros = True,\n",
    "     use_direction_classifier   = True,\n",
    "     use_groupnorm              = False,\n",
    "     num_groups                 = 32,\n",
    "     use_bev                    = False,\n",
    "     box_code_size              = 7,\n",
    ")\n",
    "checkpoint_loc = \"/workspace/finn/pp_net_params/rpn_weights\"\n",
    "checkpoint = torch.load(checkpoint_loc, map_location=\"cpu\")\n",
    "rpn.load_state_dict(checkpoint)\n",
    "rpn = rpn.eval()\n",
    "bo.export_finn_onnx(rpn, in_shape, build_dir + \"/{}.onnx\".format(base_file_name))\n",
    "\n",
    "model = ModelWrapper(build_dir + \"/{}.onnx\".format(base_file_name))\n",
    "model = model.transform(DoubleToSingleFloat())\n",
    "model = model.transform(InferShapes())\n",
    "model = model.transform(FoldConstants())\n",
    "model = model.transform(GiveUniqueNodeNames())\n",
    "model = model.transform(GiveReadableTensorNames())\n",
    "model.save(build_dir + \"/{}_tidy.onnx\".format(base_file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showInNetron(build_dir + \"/{}_tidy.onnx\".format(base_file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.transformation.streamline import Streamline\n",
    "from finn.transformation.lower_convs_to_matmul import LowerConvsToMatMul\n",
    "from finn.transformation.bipolar_to_xnor import ConvertBipolarMatMulToXnorPopcount\n",
    "import finn.transformation.streamline.absorb as absorb\n",
    "from finn.transformation.streamline.reorder import MakeMaxPoolNHWC, MoveAddMulPastIm2Col\n",
    "\n",
    "model = ModelWrapper(build_dir + \"/{}_tidy.onnx\".format(base_file_name))\n",
    "model = model.transform(Streamline())\n",
    "model = model.transform(LowerConvsToMatMul())\n",
    "model = model.transform(MakeMaxPoolNHWC())\n",
    "model = model.transform(MoveAddMulPastIm2Col())\n",
    "model = model.transform(absorb.AbsorbTransposeIntoMultiThreshold())\n",
    "model = model.transform(ConvertBipolarMatMulToXnorPopcount())\n",
    "model = model.transform(Streamline())\n",
    "model.save(build_dir + \"/{}_streamlined.onnx\".format(base_file_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showInNetron(build_dir + \"/{}_streamlined.onnx\".format(base_file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import finn.transformation.fpgadataflow.convert_to_hls_layers as to_hls\n",
    "from finn.transformation.fpgadataflow.create_dataflow_partition import (\n",
    "    CreateDataflowPartition,\n",
    ")\n",
    "from finn.transformation.move_reshape import RemoveCNVtoFCFlatten\n",
    "from finn.custom_op.registry import getCustomOp\n",
    "\n",
    "# choose the memory mode for the MVTU units, decoupled or const\n",
    "mem_mode = \"decoupled\"\n",
    "\n",
    "model = ModelWrapper(build_dir + \"/{}_streamlined.onnx\".format(base_file_name))\n",
    "model = model.transform(to_hls.InferBinaryStreamingFCLayer(mem_mode))\n",
    "model = model.transform(to_hls.InferQuantizedStreamingFCLayer(mem_mode))\n",
    "model = model.transform(to_hls.InferConvInpGen())\n",
    "model = model.transform(to_hls.InferStreamingMaxPool())\n",
    "# get rid of Reshape(-1, 1) operation between hlslib nodes\n",
    "model = model.transform(RemoveCNVtoFCFlatten())\n",
    "parent_model = model.transform(CreateDataflowPartition())\n",
    "parent_model.save(build_dir + \"/{}_dataflow_parent.onnx\".format(base_file_name))\n",
    "sdp_node = parent_model.get_nodes_by_op_type(\"StreamingDataflowPartition\")[0]\n",
    "sdp_node = getCustomOp(sdp_node)\n",
    "dataflow_model_filename = sdp_node.get_nodeattr(\"model\")\n",
    "# save the dataflow partition with a different name for easier access\n",
    "dataflow_model = ModelWrapper(dataflow_model_filename)\n",
    "dataflow_model.save(build_dir + \"/{}_dataflow_model.onnx\".format(base_file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showInNetron(build_dir + \"/{}_dataflow_parent.onnx\".format(base_file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbg = True\n",
    "if dbg:\n",
    "    import finn.custom_op.registry as registry\n",
    "    import numpy as np\n",
    "    for op_type in [\"Mul\", \"Add\", \"MultiThreshold\"]:\n",
    "    \n",
    "        model = ModelWrapper(build_dir + \"/{}_dataflow_parent.onnx\".format(base_file_name))\n",
    "        nodes = model.get_nodes_by_op_type(op_type)\n",
    "        node = nodes[0]\n",
    "        \n",
    "        if op_type == \"MultiThreshold\":\n",
    "            inst = registry.custom_op[op_type](node)\n",
    "            thresholds = model.get_initializer(node.input[1])\n",
    "            out_scale  = inst.get_nodeattr(\"out_scale\")\n",
    "            out_bias   = inst.get_nodeattr(\"out_bias\")\n",
    "            data_layout = inst.get_nodeattr(\"data_layout\")\n",
    "            print(\"data layout (if other than NCHW, then check MultThreshold class code): {}\".format(data_layout))\n",
    "            print(\"out_scale: {}\".format(type(out_scale), out_scale))\n",
    "            print(\"out_bias: {}\".format(type(out_bias), out_bias))\n",
    "            np.save(\"{}/pp_net_params/thresholds.npy\".format(build_dir), thresholds)\n",
    "        elif op_type == \"Add\":\n",
    "            tensor = model.get_initializer(node.input[1])\n",
    "            print(tensor.shape)\n",
    "            np.save(\"{}/pp_net_params/add_params.npy\".format(build_dir), tensor)\n",
    "        elif op_type == \"Mul\":\n",
    "            tensor = model.get_initializer(node.input[1])\n",
    "            print(tensor.shape)\n",
    "            np.save(\"{}/pp_net_params/mul_params.npy\".format(build_dir), tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "showInNetron(build_dir + \"/{}_dataflow_model.onnx\".format(base_file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.transformation.fpgadataflow.insert_dwc import InsertDWC\n",
    "from finn.transformation.fpgadataflow.insert_tlastmarker import InsertTLastMarker\n",
    "from finn.transformation.fpgadataflow.insert_fifo import InsertFIFO\n",
    "from finn.transformation.fpgadataflow.annotate_resources import AnnotateResources\n",
    "\n",
    "model = ModelWrapper(build_dir + \"/{}_dataflow_model.onnx\".format(base_file_name))\n",
    "fc_layers = model.get_nodes_by_op_type(\"StreamingFCLayer_Batch\")\n",
    "\n",
    "# each tuple is (PE, SIMD, in_fifo_depth, ram_style) for a layer\n",
    "# there are 13 StreamingFCLayer_Batch\n",
    "PEs   = 16\n",
    "SIMDs = 16\n",
    "FIFOs = 256\n",
    "folding = [\n",
    "    (PEs, SIMDs, FIFOs, \"block\"), #0\n",
    "    (PEs, SIMDs, FIFOs, \"block\"),\n",
    "    (PEs, SIMDs, FIFOs, \"block\"),\n",
    "    (PEs, SIMDs, FIFOs, \"block\"),\n",
    "    (PEs, SIMDs, FIFOs, \"block\"), #4\n",
    "    \n",
    "    (PEs, SIMDs, FIFOs, \"block\"), #5\n",
    "    (PEs, SIMDs, FIFOs, \"block\"),\n",
    "    (PEs, SIMDs, FIFOs, \"block\"),\n",
    "    (PEs, SIMDs, FIFOs, \"block\"),\n",
    "    (PEs, SIMDs, FIFOs, \"block\"), #9\n",
    "    \n",
    "    (PEs, SIMDs, FIFOs, \"block\"), #10\n",
    "    (PEs, SIMDs, FIFOs, \"block\"),\n",
    "    (1,   SIMDs, FIFOs, \"block\"),\n",
    "]\n",
    "for fcl, (pe, simd, ififodepth, ram_style) in zip(fc_layers, folding):\n",
    "    fcl_inst = getCustomOp(fcl)\n",
    "    fcl_inst.set_nodeattr(\"PE\", pe)\n",
    "    fcl_inst.set_nodeattr(\"SIMD\", simd)\n",
    "    fcl_inst.set_nodeattr(\"inFIFODepth\", ififodepth)\n",
    "    fcl_inst.set_nodeattr(\"ram_style\", ram_style)\n",
    "    \n",
    "\n",
    "# use same SIMD values for the sliding window operators\n",
    "swg_layers = model.get_nodes_by_op_type(\"ConvolutionInputGenerator\")\n",
    "for i in range(len(swg_layers)):\n",
    "    swg_inst = getCustomOp(swg_layers[i])\n",
    "    simd = folding[i][1]\n",
    "    swg_inst.set_nodeattr(\"SIMD\", simd)\n",
    "\n",
    "model = model.transform(InsertDWC())\n",
    "model = model.transform(InsertFIFO())\n",
    "model = model.transform(InsertTLastMarker())\n",
    "model = model.transform(GiveUniqueNodeNames())\n",
    "model = model.transform(AnnotateResources(\"estimate\"))\n",
    "model.save(build_dir + \"/{}_folded.onnx\".format(base_file_name))\n",
    "print(\"Estimation of used resources: {}\".format(model.get_metadata_prop(\"res_total_estimate\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showInNetron(build_dir + \"/{}_folded.onnx\".format(base_file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.transformation.fpgadataflow.prepare_ip import PrepareIP\n",
    "from finn.transformation.fpgadataflow.hlssynth_ip import HLSSynthIP\n",
    "from finn.util.basic import pynq_part_map\n",
    "import time\n",
    "import traceback\n",
    "\n",
    "test_pynq_board = \"ZCU104\"\n",
    "test_fpga_part = pynq_part_map[test_pynq_board]\n",
    "target_clk_ns = 5\n",
    "\n",
    "time_start = time.time()\n",
    "try:\n",
    "    model = ModelWrapper(build_dir + \"/{}_folded.onnx\".format(base_file_name))\n",
    "    print(\"PrepareIP started!\")\n",
    "    model = model.transform(PrepareIP(test_fpga_part, target_clk_ns))\n",
    "    print(\"Preparing IP took {:.2f}\".format(time.time() - time_start))\n",
    "    time_start = time.time()\n",
    "    print(\"HLSSynth started!\")\n",
    "    model = model.transform(HLSSynthIP(0))\n",
    "    model.save(build_dir + \"/{}_ipgen.onnx\".format(base_file_name))\n",
    "except Exception as e:\n",
    "    print(\"Exception: {}\\n {}\".format(e, traceback.format_exc()))\n",
    "print(\"HLS Synthesis took {:.2f} seconds\".format(time.time() - time_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelWrapper(build_dir + \"/{}_ipgen.onnx\".format(base_file_name))\n",
    "model = model.transform(AnnotateResources(\"hls\"))\n",
    "print(\"Estimation of used resources (HLS): {}\".format(model.get_metadata_prop(\"res_total_hls\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the HLS synthesis is complete, we can stitch together the generated IP blocks into a larger IP that is the implementation of our network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.transformation.fpgadataflow.replace_verilog_relpaths import (\n",
    "    ReplaceVerilogRelPaths,\n",
    ")\n",
    "from finn.transformation.fpgadataflow.create_stitched_ip import CreateStitchedIP\n",
    "\n",
    "model = ModelWrapper(build_dir + \"/{}_ipgen.onnx\".format(base_file_name))\n",
    "model = model.transform(ReplaceVerilogRelPaths())\n",
    "model = model.transform(CreateStitchedIP(test_fpga_part))\n",
    "model.save(build_dir + \"/{}_ipstitch.onnx\".format(base_file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.transformation.fpgadataflow.make_pynq_proj import MakePYNQProject\n",
    "from finn.transformation.fpgadataflow.synth_pynq_proj import SynthPYNQProject\n",
    "\n",
    "model = ModelWrapper(build_dir + \"/{}_ipstitch.onnx\".format(base_file_name))\n",
    "model = model.transform(MakePYNQProject(test_pynq_board))\n",
    "vivado_proj = model.get_metadata_prop(\"vivado_pynq_proj\")\n",
    "print(\"Vivado synthesis project is at %s/resizer.xpr\" % vivado_proj)\n",
    "model.save(build_dir + \"/{}_pynqproj.onnx\".format(base_file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelWrapper(build_dir + \"/{}_pynqproj.onnx\".format(base_file_name))\n",
    "time_start = time.time()\n",
    "try:\n",
    "    model = model.transform(SynthPYNQProject())\n",
    "    model.save(build_dir + \"/{}_synth.onnx\".format(base_file_name))\n",
    "except Exception as e:\n",
    "    print(\"Exception: {}\\n {}\".format(e, traceback.format_exc()))\n",
    "print(\"Vivado project Synthesis took {:.2f} seconds\".format(time.time() - time_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from finn.transformation.fpgadataflow.make_pynq_driver import MakePYNQDriver\n",
    "from finn.transformation.fpgadataflow.make_deployment import DeployToPYNQ\n",
    "from finn.util.basic import make_build_dir\n",
    "from finn.util.visualization import showInNetron\n",
    "from finn.core.modelwrapper import ModelWrapper\n",
    "from finn.custom_op.registry import getCustomOp\n",
    "build_dir = \"/workspace/finn\"\n",
    "base_file_name = \"rpn\"\n",
    "\n",
    "# set up the following values according to your own environment\n",
    "# FINN will use ssh to deploy and run the generated accelerator\n",
    "# ip = os.getenv(\"PYNQ_IP\", \"192.168.1.99\")\n",
    "ip = \"192.168.2.99\"\n",
    "username = os.getenv(\"PYNQ_USERNAME\", \"xilinx\")\n",
    "password = os.getenv(\"PYNQ_PASSWORD\", \"xilinx\")\n",
    "port = os.getenv(\"PYNQ_PORT\", 22)\n",
    "target_dir = os.getenv(\"PYNQ_TARGET_DIR\", \"/home/xilinx/finn\")\n",
    "\n",
    "model = ModelWrapper(build_dir + \"/{}_synth.onnx\".format(base_file_name))\n",
    "model = model.transform(MakePYNQDriver())\n",
    "model = model.transform(DeployToPYNQ(ip, port, username, password, target_dir))\n",
    "deploy_dir = model.get_metadata_prop(\"pynq_deploy_dir\")\n",
    "model.save(build_dir + \"/{}_pynq_deploy.onnx\".format(base_file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sshpass -p {password} ssh {username}@{ip} -p {port} 'ls -l {target_dir}/*'\n",
    "print(deploy_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkg_resources as pk\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x = np.ones((1,64,320,320)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# point to the PYNQ-deployed model as the StreamingDataflowPartition in the parent\n",
    "parent_model = ModelWrapper(build_dir+\"/{}_dataflow_parent.onnx\".format(base_file_name))\n",
    "sdp_node = parent_model.get_nodes_by_op_type(\"StreamingDataflowPartition\")[0]\n",
    "sdp_node = getCustomOp(sdp_node)\n",
    "sdp_node.set_nodeattr(\"model\", build_dir + \"/rpn_pynq_deploy.onnx\")\n",
    "parent_model.save(build_dir+\"/rpn_dataflow_parent_with_remote_bitfile_exec.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from finn.core.onnx_exec import execute_onnx\n",
    "iname = parent_model.graph.input[0].name\n",
    "oname = parent_model.graph.output[0].name\n",
    "ishape = parent_model.get_tensor_shape(iname)\n",
    "input_dict = {iname: x.reshape(ishape)}\n",
    "parent_model.set_metadata_prop(\"pynq_ip\", ip)\n",
    "parent_model.set_metadata_prop(\"pynq_port\", str(port))\n",
    "parent_model.set_metadata_prop(\"pynq_username\", username)\n",
    "parent_model.set_metadata_prop(\"pynq_password\", password)\n",
    "parent_model.set_metadata_prop(\"pynq_target_dir\", target_dir)\n",
    "parent_model.set_metadata_prop(\"pynq_deploy_dir\", deploy_dir)\n",
    "print(parent_model.get_metadata_prop(\"pynq_ip\"))\n",
    "print(parent_model.get_metadata_prop(\"pynq_port\"))\n",
    "print(parent_model.get_metadata_prop(\"pynq_username\"))\n",
    "print(parent_model.get_metadata_prop(\"pynq_password\"))\n",
    "print(parent_model.get_metadata_prop(\"pynq_target_dir\"))\n",
    "print(parent_model.get_metadata_prop(\"pynq_deploy_dir\"))\n",
    "ret = execute_onnx(parent_model, input_dict, True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
